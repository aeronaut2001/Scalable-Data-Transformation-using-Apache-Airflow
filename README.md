# Scalable Data Transformation using Apache Airflow

 <p align="left"> <img src="https://komarev.com/ghpvc/?username=aeronaut2001&label=Profile%20views&color=0e75b6&style=flat" alt="aeronaut2001" /> </p>
 
[![View My Profile](https://img.shields.io/badge/View-My_Profile-green?logo=GitHub)](https://github.com/aeronaut2001) 
 [![View Repositories](https://img.shields.io/badge/View-My_Repositories-blue?logo=GitHub)](https://github.com/aeronaut2001?tab=repositories)

---

## Developed ETL pipeline using Apache Airflow 
üìù Gain the skills 
---

 <h3 align="left">Languages and Tools:</h3>

<p align="left"> Cloud: </p>

<a href="https://cloud.google.com" target="_blank" rel="noreferrer"> <img src="https://www.vectorlogo.zone/logos/google_cloud/google_cloud-icon.svg" alt="gcp" width="40" height="40"/> </a> </p>

<p align="left"> Version Control System: </p>

 <a href="https://git-scm.com/" target="_blank" rel="noreferrer"> <img src="https://www.vectorlogo.zone/logos/git-scm/git-scm-icon.svg" alt="git" width="40" height="40"/> </a> </p>

<p align="left"> Programming Language - PYTHON: </p>
    <a href="https://www.python.org" target="_blank" rel="noreferrer"> <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/python/python-original.svg" alt="python" width="40" height="40"/> </a> 

<p align="left"> BIG DATA TOOL AND SOFTWARES: </p> 
  <a href="https://hadoop.apache.org/" target="_blank" rel="noreferrer"> <img src="https://www.vectorlogo.zone/logos/apache_hadoop/apache_hadoop-icon.svg" alt="hadoop" width="40" height="40"/> </a> 
  <a href="https://hive.apache.org" target="_blank" rel="noreferrer"> <img src="https://upload.wikimedia.org/wikipedia/commons/b/bb/Apache_Hive_logo.svg" alt="Apache Hive" width="40" height="40"/> </a> 
  <a href="https://spark.apache.org" target="_blank" rel="noreferrer"> <img src="https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg" alt="Apache Spark" width="40" height="40"/> </a> 
<a href="https://www.linux.org/" target="_blank" rel="noreferrer"> <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/linux/linux-original.svg" alt="linux" width="40" height="40"/> </a> </p>

<p align="left"> WORKFLOW MANAGEMENT: </p> 
<a href="https://airflow.apache.org" target="_blank" rel="noreferrer"> <img src="https://upload.wikimedia.org/wikipedia/commons/d/de/AirflowLogo.png" alt="Apache Airflow" width="40" height="40"/> </a></p>
 
 ---

## üìô Project Structures :

- [x] **Project Introduction:**
- Implemented an end-to-end Extract, Transform, Load (ETL) pipeline leveraging Apache Airflow and Google Cloud Dataproc. Orchestrated the execution of Spark jobs for data processing, enabling seamless handling and transformation of large-scale datasets. Utilized Airflow's scheduling capabilities to automate the workflow, ensuring timely and efficient data processing.
- [x] **ETL Pipeline Development:**
- Created a data pipeline (Extract, Transform, Load) using Apache Airflow and Google Cloud Dataproc.
- Designed it to smoothly handle moving, changing, and loading large amounts of data.
- [x] **Spark Job Management:**
- Managed Spark jobs to process vast datasets efficiently within the pipeline.
- Used Spark's power for handling big data transformations and computations.
- [x] **Automated Workflow with Airflow:**
- Automated tasks' timing and execution using Apache Airflow's scheduling features.
- Ensured accurate and timely data processing by scheduling tasks effectively.
- [x] **Cloud-Based Expertise:**
- Applied cloud-based solutions by deploying the pipeline on Google Cloud Platform.
- Utilized Dataproc's scalability to manage and execute Spark jobs seamlessly.
- [x] **Impact and Achievements:**
- Enhanced data processing efficiency and reliability through optimized workflows.
- Demonstrated proficiency in ETL design, Spark job orchestration, and cloud-based workflow automation.
- [x] **The PySpark script provided processes CSV data by:**
  - Initializing a SparkSession for interaction with Spark.
  - Defining paths for input/output data on Google Cloud Storage (GCS).
  - Reading CSV files into Spark DataFrames with inferred schema and headers.
  - Performing data operations like filtering .
  - Writing the transformed data back to GCS in a CSV file ('joined_output.csv').
- [x] **Conclusion:**
- This project exemplifies the successful construction of ETL pipeline using Apache Airflow and Google Cloud Dataproc. The focus on efficient Spark job management and automated workflows underscores its capability in handling extensive data tasks seamlessly. Leveraging cloud-based solutions highlighted expertise in optimizing data workflows, resulting in enhanced processing efficiency and reliability. This project showcases prowess in ETL design, Spark job orchestration, and cloud-based automation, significantly streamlining data processing within a scalable environment.
